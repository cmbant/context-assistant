{
  "systemPrompt": "You provide expert help on using the {PROGRAM_NAME} Python package.\nWhen showing code examples, use Python 3.11+ syntax, follow best practices and use line length of 79 characters.\n\nOnly answer questions related to {PROGRAM_ID} based on the provided documentation: if you don't know the answer or if it's not covered in the documentation, say so. But you can use general knowledge, cosmology/python/matplotlib and other technical expertise to help with less specific queries and coding.\n\nWhen explaining mathematical concepts or equations, use LaTeX syntax (e.g., $\\alpha^2$ for inline math or $$E = mc^2$$ for display math). Properly format all mathematical expressions, variables, and equations using LaTeX notation.",
  "programs": [
    {
      "id": "camb",
      "name": "CAMB",
      "description": "Code for Anisotropies in the Microwave Background",
      "contextFiles": [],
      "combinedContextFile": "https://camb.readthedocs.io/en/latest/_static/camb_docs_combined.md",
      "docsUrl": "https://camb.readthedocs.io/",
      "extraSystemPrompt" : "When you need default camb params for example code, you can use \"pars = camb.read_ini('https://tinyurl.com/planck2018')\"."
    },
    {
      "id": "getdist",
      "name": "GetDist",
      "description": "Python package for analysing Monte Carlo samples",
      "contextFiles": [],
      "combinedContextFile": "https://getdist.readthedocs.io/en/latest/_static/getdist_docs_combined.md",
      "docsUrl": "https://getdist.readthedocs.io/",
      "extraSystemPrompt": "getdist should use ignore_rows to remove burn in when samples come from MCMC chains without burn in already removed."
    },
    {
      "id": "cobaya",
      "name": "Cobaya",
      "description": "Monte Carlo parameter estimation",
      "combinedContextFile": "https://cobaya.readthedocs.io/en/latest/_static/cobaya_docs_combined.md",
      "docsUrl": "https://cobaya.readthedocs.io/"
    }
  ],
  "defaultProgram": "camb",
  "showContextLink": true,
  "simpleMode": false,
  "greeting": "How can I help you?",
  "defaultModelId": "gemini/gemini-flash-latest",
  "fallbackModelId": "openai/gpt-5-mini",
  "useDirectOpenAIKey": true,
  "useDirectGeminiKey": true,
  "availableModels": [
    {
      "id": "gemini/gemini-flash-latest",
      "name": "Gemini Flash",
      "description": "Latest Gemini Flash model",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "openai/gpt-5-mini",
      "name": "GPT-5 Mini",
      "description": "OpenAI GPT-5 Mini model",
      "options": {
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324",
      "name": "Deepseek Chat V3",
      "description": "Deepseek's powerful chat model via OpenRouter",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    }
  ]
}
