{
  "systemPrompt": "You provide expert help on using the {PROGRAM_NAME} Python package.\nWhen showing code examples, use Python 3.11+ syntax, follow best practices and use line length of 79 characters.\n\nOnly answer questions related to {PROGRAM_ID} based on the provided documentation.\nIf you don't know the answer or if it's not covered in the documentation, say so, but you can use general knowledge and cosmology expertise to help with less specific queries.\n\nWhen explaining mathematical concepts or equations, use LaTeX syntax (e.g., $\\alpha^2$ for inline math or $$E = mc^2$$ for display math). Properly format all mathematical expressions, variables, and equations using LaTeX notation.",
  "programs": [
    {
      "id": "camb",
      "name": "CAMB",
      "description": "Code for Anisotropies in the Microwave Background",
      "contextFiles": [],
      "combinedContextFile": "https://camb.readthedocs.io/en/latest/_static/camb_docs_combined.md",
      "docsUrl": "https://camb.readthedocs.io/",
      "extraSystemPrompt" : "When you need default camb params for example code, you can use \"pars = camb.read_ini('https://tinyurl.com/planck2018')\"."
    },
    {
      "id": "getdist",
      "name": "GetDist",
      "description": "Python package for analysing Monte Carlo samples",
      "contextFiles": [],
      "combinedContextFile": "https://getdist.readthedocs.io/en/latest/_static/getdist_docs_combined.md",
      "docsUrl": "https://getdist.readthedocs.io/",
      "extraSystemPrompt": "getdist should use ignore_rows to remove burn in when samples come from MCMC chains without burn in already removed."
    },
    {
      "id": "cobaya",
      "name": "Cobaya",
      "description": "Monte Carlo parameter estimation",
      "combinedContextFile": "https://cobaya.readthedocs.io/en/latest/_static/cobaya_docs_combined.md",
      "docsUrl": "https://cobaya.readthedocs.io/"
    }
  ],
  "defaultProgram": "camb",
  "showContextLink": true,
  "simpleMode": false,
  "greeting": "How can I help you?",
  "defaultModelId": "gemini/gemini-2.5-flash-preview-04-17",
  "fallbackModelId": "openai/gpt-4.1-mini-2025-04-14",
  "useDirectOpenAIKey": true,
  "useDirectGeminiKey": true,
  "availableModels": [
    {
      "id": "gemini/gemini-2.5-flash-preview-04-17",
      "name": "Gemini Flash",
      "description": "Latest Gemini Flash model",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "gemini/gemini-2.5-pro-exp-03-25",
      "name": "Gemini 2.5 Pro",
      "description": "More powerful model for complex queries",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "openai/gpt-4.1-mini-2025-04-14",
      "name": "GPT-4.1 Mini",
      "description": "OpenAI GPT-4.1 Mini model",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "openai/o4-mini",
      "name": "o4 Mini",
      "description": "OpenAI thinking model",
      "options": {
        "temperature": 1,
        "max_completion_tokens": 4096,
        "stream": true
      }
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324",
      "name": "Deepseek Chat V3",
      "description": "Deepseek's powerful chat model via OpenRouter",
      "options": {
        "temperature": 0.7,
        "max_completion_tokens": 4096,
        "stream": true
      }
    }
  ]
}
